{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34a1e8f6-a63b-466c-97cc-f946c4823c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import os\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from geopy.distance import geodesic \n",
    "import math\n",
    "from multiprocess import Pool\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "544de285-1340-4647-9f99-8a889029fd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## my own directory\n",
    "os.chdir(\"/g/data/k10/dl6968/Semi-variogram_AU/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d55f7851-5f56-4381-80f1-fe958e268f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "### functions \n",
    "def find_neighbour_stations(df, df_lat, df_lon, df_id, center_lat, center_lon, search_radius=350):\n",
    "    '''\n",
    "    find stations within a given radius\n",
    "    '''\n",
    "    search_stations = []\n",
    "\n",
    "    for i in range(0,len(df)):\n",
    "        station  = (df_lat.iloc[i], df_lon.iloc[i])\n",
    "        distance = geodesic((center_lat,center_lon), station).kilometers\n",
    "    \n",
    "        if distance <= search_radius:\n",
    "                search_stations.append(df_id.iloc[i])\n",
    "\n",
    "    local_df = df[df_id.isin(search_stations)]    \n",
    "    \n",
    "    return local_df\n",
    "\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    '''\n",
    "    calculate distance in km from lats and lons\n",
    "    '''\n",
    "    # Radius of the Earth in kilometers\n",
    "    R = 6371.0\n",
    "    \n",
    "    # Convert latitude and longitude from degrees to radians\n",
    "    lat1 = math.radians(lat1)\n",
    "    lon1 = math.radians(lon1)\n",
    "    lat2 = math.radians(lat2)\n",
    "    lon2 = math.radians(lon2)\n",
    "    \n",
    "    # Difference in latitudes and longitudes\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    \n",
    "    # Haversine formula\n",
    "    a = math.sin(dlat / 2)**2 + math.cos(lat1) * math.cos(lat2) * math.sin(dlon / 2)**2\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
    "    \n",
    "    # Distance in kilometers\n",
    "    distance = R * c\n",
    "    \n",
    "    return distance\n",
    "    \n",
    "def distance_stn(station_id):\n",
    "    '''\n",
    "    calculate distance between two stations\n",
    "    '''\n",
    "    lat2 = df_search[df_search[\"ID\"] == station_id][\"Latitude\"].values[0]\n",
    "    lon2 = df_search[df_search[\"ID\"] == station_id][\"Longitude\"].values[0]\n",
    "                    \n",
    "    local_distance = haversine(cent_lat, cent_lon, lat2, lon2)\n",
    "    return local_distance\n",
    "\n",
    "def bearing_stn(station_id):\n",
    "    '''\n",
    "    calculate bearing between a neighbour station and the center station\n",
    "    '''\n",
    "    lat2 = df_search[df_search[\"ID\"] == station_id][\"Latitude\"].values[0]\n",
    "    lon2 = df_search[df_search[\"ID\"] == station_id][\"Longitude\"].values[0]\n",
    "                    \n",
    "    local_bearing = calculate_bearing(cent_lat, cent_lon, lat2, lon2)\n",
    "    return local_bearing\n",
    "    \n",
    "def calculate_bearing(lat1, lon1, lat2, lon2):\n",
    "    '''\n",
    "    calculate bearing with given lats and lons\n",
    "    '''\n",
    "    # Convert latitude and longitude from degrees to radians\n",
    "    lat1 = math.radians(lat1)\n",
    "    lon1 = math.radians(lon1)\n",
    "    lat2 = math.radians(lat2)\n",
    "    lon2 = math.radians(lon2)\n",
    "\n",
    "    # Calculate the difference in longitude\n",
    "    delta_lon = lon2 - lon1\n",
    "\n",
    "    # Calculate the bearing using the formula\n",
    "    x = math.sin(delta_lon) * math.cos(lat2)\n",
    "    y = math.cos(lat1) * math.sin(lat2) - math.sin(lat1) * math.cos(lat2) * math.cos(delta_lon)\n",
    "    \n",
    "    initial_bearing = math.atan2(x, y)\n",
    "\n",
    "    # Convert from radians to degrees\n",
    "    initial_bearing = math.degrees(initial_bearing)\n",
    "\n",
    "    # Normalize the bearing to 0-360 degrees\n",
    "    compass_bearing = (initial_bearing + 360) % 360\n",
    "\n",
    "    return compass_bearing\n",
    "\n",
    "def calc_p99(ds,var_name=None):\n",
    "    precip_da = ds\n",
    "\n",
    "    if var_name!=None:\n",
    "        precip_da = ds[var_name].load()\n",
    "    filtered_da = precip_da.where(precip_da > 1, drop=True)\n",
    "    # Compute the 99th percentile along the time dimension\n",
    "    p99 = filtered_da.quantile(percentile, dim='time')\n",
    "    \n",
    "    # Find the datetimes where the rain rate is higher than the 99th percentile\n",
    "    rain_rate_above_p99 = precip_da > p99\n",
    "    \n",
    "    # Get the datetimes where rain rate is higher than the 99th percentile\n",
    "    datetimes_above_p99 = ds['time'][rain_rate_above_p99]\n",
    "\n",
    "    return p99, datetimes_above_p99\n",
    "\n",
    "def find_val_extreme(station_id):\n",
    "    '''\n",
    "    identify whether an extreme day for the station at the semi-variogram center\n",
    "    is also an extreme day for a neighbour station\n",
    "    '''\n",
    "    search_id = str(station_id).zfill(6)\n",
    "    ds_search = xr.open_dataset(f\"/g/data/w40/dl6968/BoM_daily_stations/percentiles/{search_id}.nc\")\n",
    "    precip_da = ds_search[\"prcp\"]\n",
    "    stn_p90 = df_pc[df_pc[\"ID\"]==station_id][pc_str].values[0]\n",
    "    if stn_p90>0:\n",
    "    ## I've already filled in all the no data days with -1\n",
    "        extreme_val = precip_da.sel(time=extreme_dates, method=\"nearest\")\n",
    "        extreme_pc = ds_search[\"percentile\"].sel(time=extreme_dates, method=\"nearest\")\n",
    "        ## flag NaN days with -1, below p90 days 0, and take the values for above p90 days\n",
    "        extreme_flag =  extreme_val.copy()\n",
    "        extreme_flag = extreme_flag.where(extreme_val>=stn_p90, 0)\n",
    "        extreme_flag = extreme_flag.where(extreme_val!=-1, -1)\n",
    "        return extreme_val.values.flatten(), extreme_flag.values.flatten(), extreme_pc.values.flatten()\n",
    "    else:\n",
    "        extreme_val = np.zeros((len(extreme_dates), 1))  # Local array for this station\n",
    "        extreme_flag = np.zeros((len(extreme_dates), 1))  # This is a flag with NaN as -1, <p90 as 0, and >=p90 as value\n",
    "        # ds_pc = xr.open_dataset(f\"/g/data/w40/dl6968/BoM_daily_stations/percentiles/{search_id}.nc\")\n",
    "        extreme_pc = np.zeros((len(extreme_dates), 1)) \n",
    "        extreme_pc[:] = -1\n",
    "        extreme_val[:] = -1\n",
    "        extreme_flag[:] = -1\n",
    "        return extreme_val.flatten(), extreme_flag.flatten(), extreme_pc.flatten()\n",
    "        \n",
    "def calc_gamma(zx1, zxh):\n",
    "    '''\n",
    "    Equation 1 in Touma et al. (2018)\n",
    "    '''\n",
    "    gamma = 0.5*(zx1-zxh)**2\n",
    "    return gamma\n",
    "\n",
    "def process_gamma_distance(dates):\n",
    "    '''\n",
    "    get semi-variogram for each extreme day using the flags\n",
    "    '''\n",
    "    local_N11 = np.zeros(bins.shape[0])  # Local array for N11 count per bin\n",
    "    local_N10 = np.zeros(bins.shape[0])  # Local array for N10 count per bin\n",
    "    val_arr = df_flag[dates].values\n",
    "    local_distance = df_search[\"Distance\"].values\n",
    "    local_gamma = np.zeros_like(val_arr)\n",
    "    \n",
    "    \n",
    "    ## need to have more than 20 stations in the neighbourhood\n",
    "    count = len(np.argwhere(val_arr>=0))\n",
    "    if count >= 20:\n",
    "        # Calculate distance and gamma for each station\n",
    "        flag_val = df_flag[dates].values\n",
    "        ## if flag > 0, then it's an extreme for the neighbour station\n",
    "        ## else not an extreme so Zxh=0\n",
    "        local_Zxh = np.array([1 if i > 0 else 0 for i in flag_val ])\n",
    "        local_gamma = calc_gamma(Zx1, local_Zxh)\n",
    "        ## if flag == -1, mark gamma as -1, do not include it in the semi-variogram\n",
    "        local_gamma[flag_val ==-1] = -1\n",
    "        local_Zxh[flag_val ==-1] = -1\n",
    "        ## need at least two stations in one bin to give a reasonable estimation\n",
    "        if len(np.argwhere(local_Zxh==1))>=2:\n",
    "            bin_ids = np.digitize(df_search[\"Distance\"], bins) - 1  # Get bin IDs, adjust for Python indexing\n",
    "            bin_ids = np.clip(bin_ids, 0, len(bins) - 1)  # Ensure bin IDs are valid\n",
    "            \n",
    "            # Masks for gamma values of each bin\n",
    "            ## Each neighbour station has a bin ID\n",
    "            ## mask N11 is True when local_gamma ==0, similar for N10\n",
    "            ## -1 is not counted\n",
    "            mask_N11 = (local_gamma == 0)\n",
    "            mask_N10 = (local_gamma == 0.5)\n",
    "            \n",
    "            # Count pairs for each bin using np.bincount with weights\n",
    "            local_N11 += np.bincount(bin_ids[mask_N11], minlength=len(bins))\n",
    "            local_N10 += np.bincount(bin_ids[mask_N10], minlength=len(bins))\n",
    "                \n",
    "        return dates, local_N11, local_N10, local_distance, local_gamma\n",
    "    else:\n",
    "        return dates, None , None, None, None\n",
    "\n",
    "def calc_ratio_gamma(N11,N10):\n",
    "    ratio = 0.5*(N10/(N10+N11))\n",
    "    if len(ratio.shape)==2:\n",
    "        ratio[:,0] = 0\n",
    "    if len(ratio.shape)==1:\n",
    "        ratio[0] = 0    \n",
    "    return ratio\n",
    "    \n",
    "def process_date_bins_and_stations(m):\n",
    "    dates = extreme_dates[m]\n",
    "    \n",
    "    # Preallocate and avoid repetitive dictionary creation\n",
    "    local_station_dict = {\n",
    "        \"Day\": [], \"Date\": [], \"Spec_stn\": [], \"cent_lat\": [], \"cent_lon\": [],\n",
    "        \"Neighb_stn\": [], \"lat\": [], \"lon\": [], \"distance\": [], \"angle\": [],\n",
    "        \"val\": [], \"flag\": [], \"pc\": []\n",
    "    }\n",
    "    local_bins_dict = {\n",
    "        \"Day\": [m] * len(bins),\n",
    "        \"Date\": [dates] * len(bins),\n",
    "        \"Bins\": bins.tolist(),\n",
    "        \"N11\": N11_arr[m, :].tolist(),\n",
    "        \"N10\": N10_arr[m, :].tolist(),\n",
    "        \"gamma\": ratio_arr[m, :].tolist()\n",
    "    }\n",
    "\n",
    "    # Filter valid stations first\n",
    "    valid_mask = df_val[dates] > -1\n",
    "    valid_stations = df_val[valid_mask]\n",
    "\n",
    "    # Retrieve data for all valid stations\n",
    "    station_ids = valid_stations[\"ID\"].values\n",
    "    lats = df_search.set_index(\"ID\").loc[station_ids, \"Latitude\"].values\n",
    "    lons = df_search.set_index(\"ID\").loc[station_ids, \"Longitude\"].values\n",
    "    distances = distance_arr[m, valid_mask]\n",
    "    local_bearings = df_search[\"Bearing\"].values[valid_mask]\n",
    "    values = valid_stations[dates].values\n",
    "    flags = df_flag[dates][valid_mask].values\n",
    "    pcs = df_pc_val[dates][valid_mask].values\n",
    "\n",
    "    # Append data to the station dictionary\n",
    "    local_station_dict[\"Day\"] = [m] * len(valid_stations)\n",
    "    local_station_dict[\"Date\"] = [dates] * len(valid_stations)\n",
    "    local_station_dict[\"Spec_stn\"] = [spec_id] * len(valid_stations)\n",
    "    local_station_dict[\"cent_lat\"] = [cent_lat] * len(valid_stations)\n",
    "    local_station_dict[\"cent_lon\"] = [cent_lon] * len(valid_stations)\n",
    "    local_station_dict[\"Neighb_stn\"] = station_ids.tolist()\n",
    "    local_station_dict[\"lat\"] = lats.tolist()\n",
    "    local_station_dict[\"lon\"] = lons.tolist()\n",
    "    local_station_dict[\"distance\"] = distances.tolist()\n",
    "    local_station_dict[\"angle\"] = local_bearings.tolist()\n",
    "    local_station_dict[\"val\"] = values.tolist()\n",
    "    local_station_dict[\"flag\"] = flags.tolist()\n",
    "    local_station_dict[\"pc\"] = pcs.tolist()\n",
    "\n",
    "    return local_station_dict, local_bins_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f79357bd-f557-4d10-b639-9c6cf5d95efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### main script starts here\n",
    "## find available stations \n",
    "\n",
    "df = pd.read_csv(\"./data/BoM_daily_stations.csv\")\n",
    "\n",
    "## remove stations that do not have data \n",
    "exclude_stn = []\n",
    "for stn_id in df[\"ID\"]:\n",
    "    bom_id = str(stn_id).zfill(6)\n",
    "    if not os.path.exists(f'/g/data/w40/dl6968/BoM_daily_stations/netcdf/{bom_id}.nc'):\n",
    "        exclude_stn.append(stn_id)\n",
    "    if not os.path.exists(f'/g/data/w40/dl6968/BoM_daily_stations/percentiles/{bom_id}.nc'):\n",
    "        if stn_id not in exclude_stn:\n",
    "            exclude_stn.append(stn_id)\n",
    "\n",
    "## mannually remove some faulty stations\n",
    "df = df[~df[\"ID\"].isin(exclude_stn)& (df[\"End_Year\"]>=1960) & (df['ID'] != 40592) & \\\n",
    "     (df['ID'] != 40593) & (df['ID'] != 58090)& (df['ID'] != 68002) & (df['ID'] != 64003) & (df['ID'] != 29051)\\\n",
    "   & (df['ID'] != 34050) & (df['ID']!=40646)& (df['ID']!=95009) & (df['ID']!=70041) & (df['ID']!=88089) \\\n",
    "  & (df['ID']!=68046) & (df['ID']!=40509) & (df['ID']!=68057)& (df['ID']!=63088) & (df['ID']!=68066) \\\n",
    "& (df['ID']!=43088)& (df['ID']!=86175)] \n",
    "\n",
    "# \n",
    "daily_lat = []\n",
    "daily_lon = []\n",
    "for i in range(0, len(df)):\n",
    "    daily_lat.append(df[\"Latitude\"].iloc[i])\n",
    "    daily_lon.append(df[\"Longitude\"].iloc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba3b1703-90f6-432e-aee4-2cae9d0ff7d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9861 stations in the list\n"
     ]
    }
   ],
   "source": [
    "## for stations to be a neighbour station\n",
    "df_neighbour = df[df[\"Years\"] >=10]\n",
    "## for stations to be a station to do semi-variogram\n",
    "df_spec = df[df[\"Years\"] >=20]\n",
    "id_list = list(df_spec[\"ID\"])\n",
    "print(f\"{len(id_list)} stations in the list\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07cf48db-82b7-4b64-be73-4cc30ab09494",
   "metadata": {},
   "outputs": [],
   "source": [
    "percentile = 0.90 ## this is for extreme days\n",
    "pc_str = \"P90\" ## this is for the neighbours in the dataframe\n",
    "df_pc = pd.read_csv(\"./data/BoM_stn_p90.csv\")\n",
    "max_pool = 28 ## number of CPUs for processing\n",
    "max_radius = 350"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb83478-ff67-4a63-88e7-a3080c03aa3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing station 89090\n"
     ]
    }
   ],
   "source": [
    "for spec_id in id_list:\n",
    "    csv_file = f\"./data/all_AU_p90/{spec_id}_station_list_all_events.csv\"\n",
    "    ## do semi-variogram if file not exists\n",
    "    if not os.path.exists(csv_file):\n",
    "        print(f\"Processing station {spec_id}\")\n",
    "        cent_lat, cent_lon = df[df[\"ID\"]==spec_id][\"Latitude\"].values[0],  df[df[\"ID\"]==spec_id][\"Longitude\"].values[0]\n",
    "        df_search1 = find_neighbour_stations(df_neighbour, df_neighbour[\"Latitude\"], df_neighbour[\"Longitude\"], df_neighbour['ID'], cent_lat, cent_lon, max_radius)\n",
    "        df_search = df_search1[(df_search1[\"ID\"] != spec_id) & (df_search1[\"End_Year\"]>=1960)].copy()\n",
    "        distance = []\n",
    "        bearings = []\n",
    "        for stn_id in df_search[\"ID\"]:\n",
    "            distance.append(distance_stn(stn_id))\n",
    "            bearings.append(bearing_stn(stn_id))\n",
    "        df_search[\"Distance\"] = distance\n",
    "        df_search[\"Bearing\"] = bearings\n",
    "        \n",
    "        ## if not enough neighbours\n",
    "        ## skip the station\n",
    "        if len(df_search)<20:\n",
    "            continue\n",
    "        else:\n",
    "            ds_spec = xr.open_dataset(\"/g/data/w40/dl6968/BoM_daily_stations/netcdf/\"+str(spec_id).zfill(6)+\".nc\")\n",
    "            var = \"prcp\"\n",
    "            ds_sel = ds_spec.sel(time=slice(\"1940-03-02\", \"2024-06-30\"))\n",
    "            prcp_p99, dt_p99 = calc_p99(ds_sel, var)\n",
    "            ds_sel.close()\n",
    "            ds_spec.close()\n",
    "            extreme_dates = []\n",
    "            for dates in dt_p99:\n",
    "                yymmdd = str(dates.values)[:10]\n",
    "                if yymmdd not in extreme_dates:\n",
    "                    extreme_dates.append(yymmdd)\n",
    "                    \n",
    "            print(\"Look through neighbour stations\")\n",
    "            ## find values and flag for that extreme day\n",
    "            ## search all stations within the 350 km (or any given radius)\n",
    "            with Pool(max_pool) as p:\n",
    "                pool_outputs = list(\n",
    "                    tqdm(\n",
    "                        p.imap(find_val_extreme,\n",
    "                               df_search[\"ID\"].values),\n",
    "                        total=len(df_search[\"ID\"]),\n",
    "                        position=0, leave=True\n",
    "                    )\n",
    "                )\n",
    "            p.join()\n",
    "            \n",
    "            bom_arr = np.zeros((len(extreme_dates), len(df_search) ))\n",
    "            bom_flag = np.zeros((len(extreme_dates), len(df_search) ))\n",
    "            bom_pc = np.zeros((len(extreme_dates), len(df_search) ))\n",
    "            \n",
    "            for istn in range(0, len(df_search)):\n",
    "                bom_arr[:,istn] = pool_outputs[istn][0]\n",
    "                bom_flag[:,istn] = pool_outputs[istn][1]\n",
    "                bom_pc[:,istn] = pool_outputs[istn][2]\n",
    "        \n",
    "            bom_tmp = bom_arr.copy()\n",
    "            bom_arr[np.isnan(bom_tmp)] = -1\n",
    "            \n",
    "            ## Assign the daily values for each extreme day and each neighbour station\n",
    "            ## No values should be -1\n",
    "            df_val = pd.DataFrame(bom_arr.T, index=df_search[\"ID\"], columns=extreme_dates)\n",
    "            \n",
    "            # Reset the index and add the 'ID' column\n",
    "            df_val.reset_index(inplace=True)\n",
    "            df_val.rename(columns={'index': 'ID'}, inplace=True)\n",
    "            del bom_tmp\n",
    "            \n",
    "            ## Assign the daily flags for each extreme day and each neighbour station\n",
    "            ## No values should be -1\n",
    "            ## Not an extreme is 0\n",
    "            ## Keep the precipitation value if is an extreme\n",
    "            bom_tmp = bom_flag.copy()\n",
    "            bom_flag[np.isnan(bom_tmp)] = -1\n",
    "            df_flag = pd.DataFrame(bom_flag.T, index=df_search[\"ID\"], columns=extreme_dates)\n",
    "                        \n",
    "            # Reset the index and add the 'ID' column\n",
    "            df_flag.reset_index(inplace=True)\n",
    "            df_flag.rename(columns={'index': 'ID'}, inplace=True)\n",
    "            \n",
    "            del bom_tmp\n",
    "            \n",
    "            ## Assign the daily precentiles for each extreme day and each neighbour station\n",
    "            ## only show the P90 value for the station if a station is available on the day\n",
    "            bom_pc[np.isnan(bom_pc)] = -1\n",
    "            \n",
    "            df_pc_val = pd.DataFrame(bom_pc.T, index=df_search[\"ID\"], columns=extreme_dates)\n",
    "            \n",
    "            # Reset the index and add the 'ID' column\n",
    "            df_pc_val.reset_index(inplace=True)\n",
    "            df_pc_val.rename(columns={'index': 'ID'}, inplace=True)\n",
    "        ### semi-variogram\n",
    "        bins = np.arange(5, 360, 10)\n",
    "        N11_arr = np.zeros((len(extreme_dates),bins.shape[0]))\n",
    "        N10_arr = np.zeros((len(extreme_dates),bins.shape[0]))\n",
    "        \n",
    "        distance_arr = np.zeros((len(extreme_dates), len(df_search)))\n",
    "        gamma_arr = np.zeros((len(extreme_dates), len(df_search)))\n",
    "        Zx1 = 1\n",
    "        print(\"Semi-variogram\")\n",
    "        \n",
    "        with Pool(max_pool) as p:\n",
    "            pool_outputs = list(\n",
    "                tqdm(\n",
    "                    p.imap(process_gamma_distance,\n",
    "                           extreme_dates),\n",
    "                    total=len(extreme_dates),\n",
    "                    position=0, leave=True\n",
    "                )\n",
    "            )\n",
    "        p.join()\n",
    "        \n",
    "        faulty_dates = []\n",
    "        for i in range(0, len(extreme_dates)):\n",
    "            date= pool_outputs[i][0]\n",
    "            ## make sure the dates align\n",
    "            if date == extreme_dates[i]:\n",
    "                if pool_outputs[i][1] is None:\n",
    "                    ## if results are None the data will be set to NaNs\n",
    "                    N11_arr[i, :] = np.nan\n",
    "                    N10_arr[i,:] = np.nan\n",
    "                    distance_arr[i,:] = np.nan\n",
    "                    gamma_arr[i,:] = np.nan\n",
    "                    faulty_dates.append(date)\n",
    "                else:\n",
    "                    N11_arr[i, :] = pool_outputs[i][1]\n",
    "                    N10_arr[i, :] = pool_outputs[i][2]\n",
    "                    distance_arr[i,:] = pool_outputs[i][3]\n",
    "                    gamma_arr[i,:] = pool_outputs[i][4]\n",
    "        \n",
    "        ratio_arr = calc_ratio_gamma(N11_arr,N10_arr)\n",
    "        ### save to CSV\n",
    "        # Main parallel loop\n",
    "        station_dict = {\"Day\": [], \"Date\": [], \"Spec_stn\":[], \"cent_lat\": [], \"cent_lon\":[],\"Neighb_stn\": [],\n",
    "                        \"lat\": [], \"lon\": [], \"distance\": [], \"angle\": [], \"val\": [], \"flag\": [],\"pc\":[]}\n",
    "        bins_dict = {\"Day\": [], \"Date\": [], \"Bins\": [], \"N11\": [], \"N10\": [], \"gamma\": []}\n",
    "        \n",
    "        print(\"Save to CSV\")\n",
    "        \n",
    "        with Pool(max_pool) as p:\n",
    "            pool_outputs = list(\n",
    "                tqdm(\n",
    "                    p.imap(process_date_bins_and_stations,\n",
    "                           range(0,len(extreme_dates))),\n",
    "                    total=len(extreme_dates),\n",
    "                    position=0, leave=True\n",
    "                )\n",
    "            )\n",
    "        p.join()\n",
    "        \n",
    "        for results in pool_outputs:\n",
    "            \n",
    "            for key in station_dict:\n",
    "                        station_dict[key].extend(results[0][key])\n",
    "            for key in bins_dict:\n",
    "                bins_dict[key].extend(results[1][key])\n",
    "        \n",
    "        df_stations =  pd.DataFrame.from_dict(station_dict)\n",
    "        df_bins = pd.DataFrame.from_dict(bins_dict)\n",
    "        \n",
    "        df_stations.to_csv(csv_file, index=False)\n",
    "        df_bins.to_csv(f\"./data/all_AU_p90/{spec_id}_bins_list_all_events.csv\", index=False)\n",
    "        clear_output(wait=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb47dfc-c8b5-49a6-aee0-485f35e2a630",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:myenv]",
   "language": "python",
   "name": "conda-env-myenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
