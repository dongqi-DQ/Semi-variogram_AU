{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34a1e8f6-a63b-466c-97cc-f946c4823c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import os\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from geopy.distance import geodesic \n",
    "import math\n",
    "from multiprocess import Pool\n",
    "from IPython.display import clear_output\n",
    "import warnings\n",
    "\n",
    "# Suppress all warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "544de285-1340-4647-9f99-8a889029fd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## my own directory\n",
    "os.chdir(\"/g/data/k10/dl6968/Semi-variogram_AU/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d55f7851-5f56-4381-80f1-fe958e268f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "### functions \n",
    "\n",
    "# Function to assign seasons\n",
    "def get_season(time):\n",
    "    month = time.dt.month\n",
    "    return xr.where((month == 12) | (month <= 2), 'DJF',  # Summer (Dec-Feb)\n",
    "           xr.where((month >= 3) & (month <= 5), 'MAM',  # Autumn (Mar-May)\n",
    "           xr.where((month >= 6) & (month <= 8), 'JJA',  # Winter (Jun-Aug)\n",
    "                    'SON')))    \n",
    "    \n",
    "def find_neighbour_stations(df, df_lat, df_lon, df_id, center_lat, center_lon, search_radius=350):\n",
    "    '''\n",
    "    find stations within a given radius\n",
    "    '''\n",
    "    search_stations = []\n",
    "\n",
    "    for i in range(0,len(df)):\n",
    "        station  = (df_lat.iloc[i], df_lon.iloc[i])\n",
    "        distance = geodesic((center_lat,center_lon), station).kilometers\n",
    "    \n",
    "        if distance <= search_radius:\n",
    "                search_stations.append(df_id.iloc[i])\n",
    "\n",
    "    local_df = df[df_id.isin(search_stations)]    \n",
    "    \n",
    "    return local_df\n",
    "\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    '''\n",
    "    calculate distance in km from lats and lons\n",
    "    '''\n",
    "    # Radius of the Earth in kilometers\n",
    "    R = 6371.0\n",
    "    \n",
    "    # Convert latitude and longitude from degrees to radians\n",
    "    lat1 = math.radians(lat1)\n",
    "    lon1 = math.radians(lon1)\n",
    "    lat2 = math.radians(lat2)\n",
    "    lon2 = math.radians(lon2)\n",
    "    \n",
    "    # Difference in latitudes and longitudes\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    \n",
    "    # Haversine formula\n",
    "    a = math.sin(dlat / 2)**2 + math.cos(lat1) * math.cos(lat2) * math.sin(dlon / 2)**2\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
    "    \n",
    "    # Distance in kilometers\n",
    "    distance = R * c\n",
    "    \n",
    "    return distance\n",
    "    \n",
    "def distance_stn(station_id):\n",
    "    '''\n",
    "    calculate distance between two stations\n",
    "    '''\n",
    "    lat2 = df_search[df_search[\"ID\"] == station_id][\"Latitude\"].values[0]\n",
    "    lon2 = df_search[df_search[\"ID\"] == station_id][\"Longitude\"].values[0]\n",
    "                    \n",
    "    local_distance = haversine(cent_lat, cent_lon, lat2, lon2)\n",
    "    return local_distance\n",
    "\n",
    "def bearing_stn(station_id):\n",
    "    '''\n",
    "    calculate bearing between a neighbour station and the center station\n",
    "    '''\n",
    "    lat2 = df_search[df_search[\"ID\"] == station_id][\"Latitude\"].values[0]\n",
    "    lon2 = df_search[df_search[\"ID\"] == station_id][\"Longitude\"].values[0]\n",
    "                    \n",
    "    local_bearing = calculate_bearing(cent_lat, cent_lon, lat2, lon2)\n",
    "    return local_bearing\n",
    "    \n",
    "def calculate_bearing(lat1, lon1, lat2, lon2):\n",
    "    '''\n",
    "    calculate bearing with given lats and lons\n",
    "    '''\n",
    "    # Convert latitude and longitude from degrees to radians\n",
    "    lat1 = math.radians(lat1)\n",
    "    lon1 = math.radians(lon1)\n",
    "    lat2 = math.radians(lat2)\n",
    "    lon2 = math.radians(lon2)\n",
    "\n",
    "    # Calculate the difference in longitude\n",
    "    delta_lon = lon2 - lon1\n",
    "\n",
    "    # Calculate the bearing using the formula\n",
    "    x = math.sin(delta_lon) * math.cos(lat2)\n",
    "    y = math.cos(lat1) * math.sin(lat2) - math.sin(lat1) * math.cos(lat2) * math.cos(delta_lon)\n",
    "    \n",
    "    initial_bearing = math.atan2(x, y)\n",
    "\n",
    "    # Convert from radians to degrees\n",
    "    initial_bearing = math.degrees(initial_bearing)\n",
    "\n",
    "    # Normalize the bearing to 0-360 degrees\n",
    "    compass_bearing = (initial_bearing + 360) % 360\n",
    "\n",
    "    return compass_bearing\n",
    "\n",
    "# Function to calculate 90th percentile based on months instead of seasons\n",
    "def calc_p90_datetime(ds, var_name=None):\n",
    "    ds = ds.assign_coords(month=ds['time'].dt.month)\n",
    "\n",
    "    rainy_days = ds[var_name].where(ds[var_name] > 1, drop=True)\n",
    "    p90_monthly = rainy_days.groupby('month').quantile(percentile, dim='time', skipna=True)\n",
    "    \n",
    "    # Find days where precipitation > P90 for the corresponding month and station\n",
    "    exceeds_p90 = ds[var_name].groupby('time.month') >= p90_monthly\n",
    "    \n",
    "    # Filter the dataset to include only days where precipitation > P90\n",
    "    ds_exceeds_p90 = ds.where(exceeds_p90)\n",
    "    ds_extreme = ds_exceeds_p90.dropna(dim='time', how='all')\n",
    "    datetimes_above_p90 = ds_extreme[\"time\"]\n",
    "    return datetimes_above_p90  \n",
    "\n",
    "# Function to find extreme values based on monthly percentiles\n",
    "def find_val_extreme(station_id):\n",
    "    '''\n",
    "    Identify whether an extreme day for the station at the semi-variogram center\n",
    "    is also an extreme day for a neighbor station.\n",
    "    '''\n",
    "    search_id = str(station_id).zfill(6)\n",
    "    \n",
    "    ds_search = xr.open_dataset(f\"/g/data/k10/dl6968/BoM_daily_station/prcp_pc_ts_qc/{search_id}.nc\")\n",
    "    ds_search = ds_search.assign_coords(month=ds_search['time'].dt.month)\n",
    "\n",
    "    rainy_days = ds_search['prcp'].where(ds_search['prcp'] > 1, drop=True)\n",
    "    \n",
    "    if len(rainy_days) == 0:\n",
    "        extreme_val = np.zeros((len(extreme_dates), 1))  # Local array for this station\n",
    "        extreme_flag = np.zeros((len(extreme_dates), 1))  # Flag with NaN as -1, <p90 as 0, and >=p90 as value\n",
    "        \n",
    "        extreme_pc = np.zeros((len(extreme_dates), 1)) \n",
    "        extreme_pc[:] = -1\n",
    "        extreme_val[:] = -1\n",
    "        extreme_flag[:] = -1\n",
    "        return extreme_val.flatten(), extreme_flag.flatten(), extreme_pc.flatten()\n",
    "    \n",
    "    else:\n",
    "        p90_monthly = rainy_days.groupby('time.month').quantile(percentile, dim='time', skipna=True)\n",
    "        precip_da = ds_search[\"prcp\"].sel(time=extreme_dates)\n",
    "        extreme_val = precip_da.copy()\n",
    "        extreme_flag = precip_da.copy()\n",
    "        extreme_pc = ds_search[\"percentile\"].sel(time=extreme_dates)\n",
    "        \n",
    "        extreme_flag = extreme_flag.where(extreme_val.groupby('time.month') >= p90_monthly, 0)\n",
    "        extreme_flag = extreme_flag.where(extreme_val != -1, -1)\n",
    "        \n",
    "        \n",
    "        return extreme_val.values.flatten(), extreme_flag.values.flatten(), extreme_pc.values.flatten()\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "def process_avail_distance(dates):\n",
    "    '''\n",
    "    get distance between center and neighbor stations for plotting later\n",
    "    also set filter for valid stations\n",
    "    '''\n",
    "    val_arr = df_flag[dates].values\n",
    "    local_distance = df_search[\"Distance\"].values\n",
    "    \n",
    "    ## need to have more than 20 stations in the neighbourhood\n",
    "    count = len(np.argwhere(val_arr>=0))\n",
    "    if count >= 20:\n",
    "                \n",
    "        return dates, local_distance\n",
    "    else:\n",
    "        return dates, None \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "def process_date_stations(m):\n",
    "    dates = extreme_dates[m]\n",
    "    \n",
    "    # Preallocate and avoid repetitive dictionary creation\n",
    "    local_station_dict = {\n",
    "        \"Day\": [], \"Date\": [], \"Spec_stn\": [], \"cent_lat\": [], \"cent_lon\": [],\n",
    "        \"Neighb_stn\": [], \"lat\": [], \"lon\": [], \"distance\": [], \"angle\": [],\n",
    "        \"val\": [], \"flag\": [], \"pc\": []\n",
    "    }\n",
    "\n",
    "\n",
    "    # Filter valid stations first\n",
    "    valid_mask = df_val[dates] > -1\n",
    "    valid_stations = df_val[valid_mask]\n",
    "\n",
    "    # Retrieve data for all valid stations\n",
    "    station_ids = valid_stations[\"ID\"].values\n",
    "    lats = df_search.set_index(\"ID\").loc[station_ids, \"Latitude\"].values\n",
    "    lons = df_search.set_index(\"ID\").loc[station_ids, \"Longitude\"].values\n",
    "    distances = distance_arr[m, valid_mask]\n",
    "    local_bearings = df_search[\"Bearing\"].values[valid_mask]\n",
    "    values = valid_stations[dates].values\n",
    "    flags = df_flag[dates][valid_mask].values\n",
    "    pcs = df_pc_val[dates][valid_mask].values\n",
    "\n",
    "    # Append data to the station dictionary\n",
    "    local_station_dict[\"Day\"] = [m] * len(valid_stations)\n",
    "    local_station_dict[\"Date\"] = [dates] * len(valid_stations)\n",
    "    local_station_dict[\"Spec_stn\"] = [spec_id] * len(valid_stations)\n",
    "    local_station_dict[\"cent_lat\"] = [cent_lat] * len(valid_stations)\n",
    "    local_station_dict[\"cent_lon\"] = [cent_lon] * len(valid_stations)\n",
    "    local_station_dict[\"Neighb_stn\"] = station_ids.tolist()\n",
    "    local_station_dict[\"lat\"] = lats.tolist()\n",
    "    local_station_dict[\"lon\"] = lons.tolist()\n",
    "    local_station_dict[\"distance\"] = distances.tolist()\n",
    "    local_station_dict[\"angle\"] = local_bearings.tolist()\n",
    "    local_station_dict[\"val\"] = values.tolist()\n",
    "    local_station_dict[\"flag\"] = flags.tolist()\n",
    "    local_station_dict[\"pc\"] = pcs.tolist()\n",
    "\n",
    "    return local_station_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f79357bd-f557-4d10-b639-9c6cf5d95efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### main script starts here\n",
    "## find available stations \n",
    "\n",
    "df = pd.read_csv(\"./data/BoM_daily_stations.csv\")\n",
    "\n",
    "## remove stations that do not have data \n",
    "exclude_stn = []\n",
    "for stn_id in df[\"ID\"]:\n",
    "    bom_id = str(stn_id).zfill(6)\n",
    "    # if not os.path.exists(f'/g/data/w40/dl6968/BoM_daily_stations/netcdf/{bom_id}.nc'):\n",
    "    #     exclude_stn.append(stn_id)\n",
    "    if not os.path.exists(f'/g/data/k10/dl6968/BoM_daily_station/prcp_pc_ts_qc/{bom_id}.nc'):\n",
    "        if stn_id not in exclude_stn:\n",
    "            exclude_stn.append(stn_id)\n",
    "\n",
    "## mannually remove some faulty stations\n",
    "df = df[~df[\"ID\"].isin(exclude_stn)& (df[\"End_Year\"]>=1960) & (df['ID'] != 40592) & \\\n",
    "     (df['ID'] != 40593) & (df['ID'] != 58090)& (df['ID'] != 68002) & (df['ID'] != 64003) & (df['ID'] != 29051)\\\n",
    "   & (df['ID'] != 34050) & (df['ID']!=40646)& (df['ID']!=95009) & (df['ID']!=70041) & (df['ID']!=88089) \\\n",
    "  & (df['ID']!=68046) & (df['ID']!=40509) & (df['ID']!=68057)& (df['ID']!=63088) & (df['ID']!=68066) \\\n",
    "& (df['ID']!=43088)& (df['ID']!=86175)] \n",
    "\n",
    "# \n",
    "daily_lat = []\n",
    "daily_lon = []\n",
    "for i in range(0, len(df)):\n",
    "    daily_lat.append(df[\"Latitude\"].iloc[i])\n",
    "    daily_lon.append(df[\"Longitude\"].iloc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba3b1703-90f6-432e-aee4-2cae9d0ff7d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9861 stations in the list\n"
     ]
    }
   ],
   "source": [
    "## for stations to be a neighbour station\n",
    "df_neighbour = df[df[\"Years\"] >=20]\n",
    "## for stations to be a station to do semi-variogram\n",
    "df_spec = df[df[\"Years\"] >=20]\n",
    "id_list = list(df_spec[\"ID\"])\n",
    "print(f\"{len(id_list)} stations in the list\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07cf48db-82b7-4b64-be73-4cc30ab09494",
   "metadata": {},
   "outputs": [],
   "source": [
    "percentile = 0.90 ## this is for extreme days\n",
    "# pc_str = \"P90\" ## this is for the neighbours in the dataframe\n",
    "df_pc = pd.read_csv(\"./data/BoM_stn_p90_season.csv\", index_col=0)\n",
    "max_pool = 28 ## number of CPUs for processing\n",
    "max_radius = 700 ## for moving neighbours\n",
    "inside_radius = 350"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bdb83478-ff67-4a63-88e7-a3080c03aa3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing station 200786\n",
      "Processing station 300017\n",
      "Processing station 200790\n",
      "Processing station 200284\n",
      "Processing station 27008\n",
      "Processing station 27009\n",
      "Processing station 300000\n",
      "Processing station 13017\n",
      "Processing station 200733\n",
      "Processing station 200440\n",
      "Processing station 200389\n",
      "Processing station 200375\n",
      "Processing station 200839\n",
      "Processing station 300004\n",
      "Processing station 200825\n",
      "Processing station 200288\n",
      "Processing station 200304\n",
      "Processing station 27018\n",
      "Processing station 13011\n",
      "Processing station 200283\n"
     ]
    }
   ],
   "source": [
    "for spec_id in id_list:\n",
    "    csv_file = f\"./data/all_AU_p90_qc/{spec_id}_station_moving_list_all_events.csv\"\n",
    "    ## do semi-variogram if file not exists\n",
    "    if not os.path.exists(csv_file):\n",
    "        print(f\"Processing station {spec_id}\")\n",
    "        cent_lat, cent_lon = df[df[\"ID\"]==spec_id][\"Latitude\"].values[0],  df[df[\"ID\"]==spec_id][\"Longitude\"].values[0]\n",
    "        df_search_moving = find_neighbour_stations(df_neighbour, df_neighbour[\"Latitude\"], df_neighbour[\"Longitude\"], df_neighbour['ID'], cent_lat, cent_lon, max_radius)\n",
    "        df_search = df_search_moving[(df_search_moving[\"ID\"] != spec_id) & (df_search_moving[\"End_Year\"]>=1960)].copy()\n",
    "        df_search_inside = find_neighbour_stations(df_neighbour, df_neighbour[\"Latitude\"], df_neighbour[\"Longitude\"], df_neighbour['ID'], cent_lat, cent_lon, inside_radius)\n",
    "        df_search_in = df_search_inside[(df_search_inside[\"ID\"] != spec_id) & (df_search_inside[\"End_Year\"]>=1960)].copy()\n",
    "        distance = []\n",
    "        bearings = []\n",
    "        for stn_id in df_search[\"ID\"]:\n",
    "            distance.append(distance_stn(stn_id))\n",
    "            bearings.append(bearing_stn(stn_id))\n",
    "        df_search[\"Distance\"] = distance\n",
    "        df_search[\"Bearing\"] = bearings\n",
    "        \n",
    "        ## if not enough neighbours\n",
    "        ## skip the station\n",
    "        if len(df_search_in)<20:\n",
    "            continue\n",
    "        else:\n",
    "            ds_spec = xr.open_dataset(\"/g/data/k10/dl6968/BoM_daily_station/prcp_pc_ts_qc/\"+str(spec_id).zfill(6)+\".nc\")\n",
    "            var = \"prcp\"\n",
    "            ds_sel = ds_spec.sel(time=slice(\"1940-03-02\", \"2024-06-30\"))\n",
    "            dt_p99 = calc_p90_datetime(ds_sel, var)\n",
    "            ds_sel.close()\n",
    "            ds_spec.close()\n",
    "            extreme_dates = []\n",
    "            for dates in dt_p99:\n",
    "                yymmdd = str(dates.values)[:10]\n",
    "                if yymmdd not in extreme_dates:\n",
    "                    extreme_dates.append(yymmdd)\n",
    "                    \n",
    "            print(\"Look through neighbour stations\")\n",
    "            ## find values and flag for that extreme day\n",
    "            ## search all stations within the 350 km (or any given radius)\n",
    "            with Pool(max_pool) as p:\n",
    "                pool_outputs = list(\n",
    "                    tqdm(\n",
    "                        p.imap(find_val_extreme,\n",
    "                               df_search[\"ID\"].values),\n",
    "                        total=len(df_search[\"ID\"]),\n",
    "                        position=0, leave=True\n",
    "                    )\n",
    "                )\n",
    "            p.join()\n",
    "            \n",
    "            bom_arr = np.zeros((len(extreme_dates), len(df_search) ))\n",
    "            bom_flag = np.zeros((len(extreme_dates), len(df_search) ))\n",
    "            bom_pc = np.zeros((len(extreme_dates), len(df_search) ))\n",
    "            \n",
    "            for istn in range(0, len(df_search)):\n",
    "                bom_arr[:,istn] = pool_outputs[istn][0]\n",
    "                bom_flag[:,istn] = pool_outputs[istn][1]\n",
    "                bom_pc[:,istn] = pool_outputs[istn][2]\n",
    "        \n",
    "            bom_tmp = bom_arr.copy()\n",
    "            bom_arr[np.isnan(bom_tmp)] = -1\n",
    "            \n",
    "            ## Assign the daily values for each extreme day and each neighbour station\n",
    "            ## No values should be -1\n",
    "            df_val = pd.DataFrame(bom_arr.T, index=df_search[\"ID\"], columns=extreme_dates)\n",
    "            \n",
    "            # Reset the index and add the 'ID' column\n",
    "            df_val.reset_index(inplace=True)\n",
    "            df_val.rename(columns={'index': 'ID'}, inplace=True)\n",
    "            del bom_tmp\n",
    "            \n",
    "            ## Assign the daily flags for each extreme day and each neighbour station\n",
    "            ## No values should be -1\n",
    "            ## Not an extreme is 0\n",
    "            ## Keep the precipitation value if is an extreme\n",
    "            bom_tmp = bom_flag.copy()\n",
    "            bom_flag[np.isnan(bom_tmp)] = -1\n",
    "            df_flag = pd.DataFrame(bom_flag.T, index=df_search[\"ID\"], columns=extreme_dates)\n",
    "                        \n",
    "            # Reset the index and add the 'ID' column\n",
    "            df_flag.reset_index(inplace=True)\n",
    "            df_flag.rename(columns={'index': 'ID'}, inplace=True)\n",
    "            \n",
    "            del bom_tmp\n",
    "            \n",
    "            ## Assign the daily precentiles for each extreme day and each neighbour station\n",
    "            ## only show the P90 value for the station if a station is available on the day\n",
    "            bom_pc[np.isnan(bom_pc)] = -1\n",
    "            \n",
    "            df_pc_val = pd.DataFrame(bom_pc.T, index=df_search[\"ID\"], columns=extreme_dates)\n",
    "            \n",
    "            # Reset the index and add the 'ID' column\n",
    "            df_pc_val.reset_index(inplace=True)\n",
    "            df_pc_val.rename(columns={'index': 'ID'}, inplace=True)\n",
    "        ### station distance for plotting later\n",
    "        \n",
    "        distance_arr = np.zeros((len(extreme_dates), len(df_search)))\n",
    "        \n",
    "        print(\"Calc station distance\")\n",
    "        \n",
    "        with Pool(max_pool) as p:\n",
    "            pool_outputs = list(\n",
    "                tqdm(\n",
    "                    p.imap(process_avail_distance,\n",
    "                           extreme_dates),\n",
    "                    total=len(extreme_dates),\n",
    "                    position=0, leave=True\n",
    "                )\n",
    "            )\n",
    "        p.join()\n",
    "        \n",
    "\n",
    "        for i in range(0, len(extreme_dates)):\n",
    "            date= pool_outputs[i][0]\n",
    "            ## make sure the dates align\n",
    "            if date == extreme_dates[i]:\n",
    "                if pool_outputs[i][1] is None:\n",
    "                    ## if results are None the data will be set to NaNs\n",
    "                    distance_arr[i,:] = np.nan\n",
    "                else:\n",
    "                   \n",
    "                    distance_arr[i,:] = pool_outputs[i][1]\n",
    "        \n",
    "        \n",
    "        ### save to CSV\n",
    "        # Main parallel loop\n",
    "        station_dict = {\"Day\": [], \"Date\": [], \"Spec_stn\":[], \"cent_lat\": [], \"cent_lon\":[],\"Neighb_stn\": [],\n",
    "                        \"lat\": [], \"lon\": [], \"distance\": [], \"angle\": [], \"val\": [], \"flag\": [],\"pc\":[]}\n",
    "\n",
    "        \n",
    "        print(\"Save to CSV\")\n",
    "        \n",
    "        with Pool(max_pool) as p:\n",
    "            pool_outputs = list(\n",
    "                tqdm(\n",
    "                    p.imap(process_date_stations,\n",
    "                           range(0,len(extreme_dates))),\n",
    "                    total=len(extreme_dates),\n",
    "                    position=0, leave=True\n",
    "                )\n",
    "            )\n",
    "        p.join()\n",
    "        \n",
    "        for results in pool_outputs:\n",
    "            \n",
    "            for key in station_dict:\n",
    "                        station_dict[key].extend(results[key])\n",
    "        \n",
    "        df_stations =  pd.DataFrame.from_dict(station_dict)\n",
    "        \n",
    "        df_stations.to_csv(csv_file, index=False)\n",
    "        \n",
    "        clear_output(wait=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c6b6eb-4c41-4bbe-88a9-bf9981c30bf6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:myenv]",
   "language": "python",
   "name": "conda-env-myenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
